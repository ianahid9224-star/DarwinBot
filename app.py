import requests
import telebot
import os

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–∑–∞–º–µ–Ω–∏ —Ç–æ–∫–µ–Ω—ã –Ω–∞ —Å–≤–æ–∏ –≤ Render) ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")   # —Ç–æ–∫–µ–Ω –æ—Ç BotFather
HF_API_KEY = os.getenv("HF_API_KEY")           # —Ç–æ–∫–µ–Ω –æ—Ç HuggingFace
HF_MODEL = os.getenv("HF_MODEL", "mistralai/Mistral-7B-Instruct-v0.2")
SYSTEM_PROMPT = os.getenv("SYSTEM_PROMPT", "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –ø–æ–Ω—è—Ç–Ω–æ –∏ –ø–æ —à–∞–≥–∞–º.")
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "400"))

bot = telebot.TeleBot(TELEGRAM_TOKEN, parse_mode="HTML")

def hf_generate(user_text: str) -> str:
    url = f"https://api-inference.huggingface.co/models/{HF_MODEL}"
    headers = {"Authorization": f"Bearer {HF_API_KEY}"}
    prompt = f"{SYSTEM_PROMPT}\n\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_text}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": MAX_NEW_TOKENS,
            "temperature": 0.7,
            "return_full_text": False
        }
    }
    r = requests.post(url, headers=headers, json=payload, timeout=120)
    data = r.json()
    if isinstance(data, list) and data and "generated_text" in data[0]:
        return data[0]["generated_text"].strip()
    return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑."

@bot.message_handler(commands=["start"])
def start(message):
    bot.reply_to(message, "–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π AI-–±–æ—Ç ü§ñ –ù–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å!")

@bot.message_handler(func=lambda m: True)
def reply(message):
    bot.send_chat_action(message.chat.id, "typing")
    answer = hf_generate(message.text)
    bot.reply_to(message, answer)

print("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
bot.infinity_polling()
